{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the reddit post flair\n",
    "\n",
    "Source: https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Manipulation \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical libraries \n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk \n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Performance Evaluation and Support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1650, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into the dataframe \n",
    "data = pd.read_csv('..\\\\data\\\\data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Title', 'Score', 'ID', 'URL', 'num_comments',\n",
       "       'created_on', 'Body', 'Original', 'Flair', 'Comments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_on</th>\n",
       "      <th>Body</th>\n",
       "      <th>Original</th>\n",
       "      <th>Flair</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you have to cook and clean all by yoursel...</td>\n",
       "      <td>283</td>\n",
       "      <td>fuhcgu</td>\n",
       "      <td>https://i.redd.it/hyd2c2b3coq41.png</td>\n",
       "      <td>15</td>\n",
       "      <td>1.585980e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Food</td>\n",
       "      <td>Not for me. I always cooked my own food. The o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCS not to layoff any employees, but no salary...</td>\n",
       "      <td>95</td>\n",
       "      <td>g2gx0s</td>\n",
       "      <td>https://www.thenewsminute.com/article/tcs-not-...</td>\n",
       "      <td>31</td>\n",
       "      <td>1.587079e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>Why not defer the increments instead of outrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How come western countries are facing so many ...</td>\n",
       "      <td>24</td>\n",
       "      <td>g41vk7</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g41vk7...</td>\n",
       "      <td>35</td>\n",
       "      <td>1.587301e+09</td>\n",
       "      <td>Having a discussion with family and I am unabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>The lockdown enforced by the government was tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXCLUSIVE: Documents Show Modi Govt Building 3...</td>\n",
       "      <td>2072</td>\n",
       "      <td>fjx9ih</td>\n",
       "      <td>https://www.huffingtonpost.in/entry/aadhaar-na...</td>\n",
       "      <td>314</td>\n",
       "      <td>1.584440e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Politics</td>\n",
       "      <td>And then came corona! To fuck up their plans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Startrail over Leh Palace - Leh Palace was bui...</td>\n",
       "      <td>215</td>\n",
       "      <td>epg8gi</td>\n",
       "      <td>https://i.redd.it/iwbeppdci3b41.jpg</td>\n",
       "      <td>6</td>\n",
       "      <td>1.579190e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Amazing to see this.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Score      ID  \\\n",
       "0  When you have to cook and clean all by yoursel...    283  fuhcgu   \n",
       "1  TCS not to layoff any employees, but no salary...     95  g2gx0s   \n",
       "2  How come western countries are facing so many ...     24  g41vk7   \n",
       "3  EXCLUSIVE: Documents Show Modi Govt Building 3...   2072  fjx9ih   \n",
       "4  Startrail over Leh Palace - Leh Palace was bui...    215  epg8gi   \n",
       "\n",
       "                                                 URL  num_comments  \\\n",
       "0                https://i.redd.it/hyd2c2b3coq41.png            15   \n",
       "1  https://www.thenewsminute.com/article/tcs-not-...            31   \n",
       "2  https://www.reddit.com/r/india/comments/g41vk7...            35   \n",
       "3  https://www.huffingtonpost.in/entry/aadhaar-na...           314   \n",
       "4                https://i.redd.it/iwbeppdci3b41.jpg             6   \n",
       "\n",
       "     created_on                                               Body  Original  \\\n",
       "0  1.585980e+09                                                NaN      True   \n",
       "1  1.587079e+09                                                NaN     False   \n",
       "2  1.587301e+09  Having a discussion with family and I am unabl...     False   \n",
       "3  1.584440e+09                                                NaN     False   \n",
       "4  1.579190e+09                                                NaN     False   \n",
       "\n",
       "              Flair                                           Comments  \n",
       "0              Food  Not for me. I always cooked my own food. The o...  \n",
       "1  Business/Finance  Why not defer the increments instead of outrig...  \n",
       "2          AskIndia  The lockdown enforced by the government was tr...  \n",
       "3          Politics     And then came corona! To fuck up their plans.   \n",
       "4       Photography                              Amazing to see this.   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Shuffling\n",
    "data[:] = data.sample(frac=1).values\n",
    "data.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1650 entries, 0 to 1649\n",
      "Data columns (total 10 columns):\n",
      "Title           1650 non-null object\n",
      "Score           1650 non-null int64\n",
      "ID              1650 non-null object\n",
      "URL             1650 non-null object\n",
      "num_comments    1650 non-null int64\n",
      "created_on      1650 non-null float64\n",
      "Body            635 non-null object\n",
      "Original        1650 non-null bool\n",
      "Flair           1650 non-null object\n",
      "Comments        1557 non-null object\n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 117.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Printing the data info to have a look at the null values and data types\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset does not have any null values for the flairs. There are null values only in Comments and Body. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the data for later use\n",
    "data_og = data.copy()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports                150\n",
       "Science/Technology    150\n",
       "[R]eddiquette         150\n",
       "Non-Political         150\n",
       "Policy/Economy        150\n",
       "AMA                   150\n",
       "Photography           150\n",
       "Business/Finance      150\n",
       "Food                  150\n",
       "Politics              150\n",
       "AskIndia              150\n",
       "Name: Flair, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Flair'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['Food' 'Policy/Economy' 'Politics' 'AskIndia' 'Photography'\n",
      " 'Business/Finance' '[R]eddiquette' 'Non-Political' 'Science/Technology'\n",
      " 'AMA' 'Sports']\n"
     ]
    }
   ],
   "source": [
    "print(len(data['Flair'].unique()))\n",
    "print(data['Flair'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of relevant features (MOVE LATER)\n",
    "features = ['Flair', 'URL', 'Title', 'Comments', 'Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flair</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food</td>\n",
       "      <td>https://www.thenewsminute.com/article/swiggy-z...</td>\n",
       "      <td>Swiggy, Zomato roll out contact-less delivery:...</td>\n",
       "      <td>The new coronavirus was viable up to 72 hours ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Policy/Economy</td>\n",
       "      <td>https://www.livemint.com/market/stock-market-n...</td>\n",
       "      <td>Rupee crashes to near record low against the U...</td>\n",
       "      <td>NRIs rejoice!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fomt9e...</td>\n",
       "      <td>Indian cops are uncontrollable animals with pe...</td>\n",
       "      <td>I don't feel wrong even if they beat anyone be...</td>\n",
       "      <td>DISCLAIMER: From the videos I have seen, I wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fjx0dq...</td>\n",
       "      <td>Is your employer making your come to office? (...</td>\n",
       "      <td>Uh. The Indian Army.   \\n\\n\\nGet back to me on...</td>\n",
       "      <td>Fill out the the form below. It is is 100% ANO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photography</td>\n",
       "      <td>https://i.redd.it/7apjmstrxnt41.jpg</td>\n",
       "      <td>Sunrise - Fatehgarh Shib, Punjab [OnePlus 7T -...</td>\n",
       "      <td>Thanks for reminding me that I haven't slept.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Flair                                                URL  \\\n",
       "0            Food  https://www.thenewsminute.com/article/swiggy-z...   \n",
       "1  Policy/Economy  https://www.livemint.com/market/stock-market-n...   \n",
       "2        Politics  https://www.reddit.com/r/india/comments/fomt9e...   \n",
       "3        AskIndia  https://www.reddit.com/r/india/comments/fjx0dq...   \n",
       "4     Photography                https://i.redd.it/7apjmstrxnt41.jpg   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Swiggy, Zomato roll out contact-less delivery:...   \n",
       "1  Rupee crashes to near record low against the U...   \n",
       "2  Indian cops are uncontrollable animals with pe...   \n",
       "3  Is your employer making your come to office? (...   \n",
       "4  Sunrise - Fatehgarh Shib, Punjab [OnePlus 7T -...   \n",
       "\n",
       "                                            Comments  \\\n",
       "0  The new coronavirus was viable up to 72 hours ...   \n",
       "1                                     NRIs rejoice!    \n",
       "2  I don't feel wrong even if they beat anyone be...   \n",
       "3  Uh. The Indian Army.   \\n\\n\\nGet back to me on...   \n",
       "4     Thanks for reminding me that I haven't slept.    \n",
       "\n",
       "                                                Body  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  DISCLAIMER: From the videos I have seen, I wil...  \n",
       "3  Fill out the the form below. It is is 100% ANO...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting the flair and ids\n",
    "data = data[features]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flair</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photography</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AMA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Non-Political</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sports</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Policy/Economy</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Flair  id\n",
       "0                 Food   0\n",
       "1     Business/Finance   1\n",
       "2             AskIndia   2\n",
       "3             Politics   3\n",
       "4          Photography   4\n",
       "5        [R]eddiquette   5\n",
       "11                 AMA   6\n",
       "26       Non-Political   7\n",
       "31              Sports   8\n",
       "40  Science/Technology   9\n",
       "44      Policy/Economy  10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning and individual id to each flair\n",
    "data['id'] = data['Flair'].factorize()[0]\n",
    "flair_category = data[['Flair', 'id']].drop_duplicates().sort_values('id')\n",
    "flair_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Food': 0, 'Business/Finance': 1, 'AskIndia': 2, 'Politics': 3, 'Photography': 4, '[R]eddiquette': 5, 'AMA': 6, 'Non-Political': 7, 'Sports': 8, 'Science/Technology': 9, 'Policy/Economy': 10}\n"
     ]
    }
   ],
   "source": [
    "# Convert into a label dctionary to be used as a means of assigning labels after the prediction\n",
    "category_labels = dict(flair_category.values)\n",
    "print(category_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Food', 1: 'Business/Finance', 2: 'AskIndia', 3: 'Politics', 4: 'Photography', 5: '[R]eddiquette', 6: 'AMA', 7: 'Non-Political', 8: 'Sports', 9: 'Science/Technology', 10: 'Policy/Economy'}\n"
     ]
    }
   ],
   "source": [
    "# Similarly, we can create an inverse of the previouus one to convert labels to categories\n",
    "category_reverse = dict(flair_category[['id', 'Flair']].values)\n",
    "print(category_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the data now. We have an id column which are basically the labels that we have to predict. They are derived from equivalent flair categories. We will be using the other columns as our input features. We will also create a series of all labels that need to predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_on</th>\n",
       "      <th>Body</th>\n",
       "      <th>Original</th>\n",
       "      <th>Flair</th>\n",
       "      <th>Comments</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you have to cook and clean all by yoursel...</td>\n",
       "      <td>283</td>\n",
       "      <td>fuhcgu</td>\n",
       "      <td>https://i.redd.it/hyd2c2b3coq41.png</td>\n",
       "      <td>15</td>\n",
       "      <td>1.585980e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Food</td>\n",
       "      <td>Not for me. I always cooked my own food. The o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCS not to layoff any employees, but no salary...</td>\n",
       "      <td>95</td>\n",
       "      <td>g2gx0s</td>\n",
       "      <td>https://www.thenewsminute.com/article/tcs-not-...</td>\n",
       "      <td>31</td>\n",
       "      <td>1.587079e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>Why not defer the increments instead of outrig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How come western countries are facing so many ...</td>\n",
       "      <td>24</td>\n",
       "      <td>g41vk7</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g41vk7...</td>\n",
       "      <td>35</td>\n",
       "      <td>1.587301e+09</td>\n",
       "      <td>Having a discussion with family and I am unabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>The lockdown enforced by the government was tr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXCLUSIVE: Documents Show Modi Govt Building 3...</td>\n",
       "      <td>2072</td>\n",
       "      <td>fjx9ih</td>\n",
       "      <td>https://www.huffingtonpost.in/entry/aadhaar-na...</td>\n",
       "      <td>314</td>\n",
       "      <td>1.584440e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Politics</td>\n",
       "      <td>And then came corona! To fuck up their plans.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Startrail over Leh Palace - Leh Palace was bui...</td>\n",
       "      <td>215</td>\n",
       "      <td>epg8gi</td>\n",
       "      <td>https://i.redd.it/iwbeppdci3b41.jpg</td>\n",
       "      <td>6</td>\n",
       "      <td>1.579190e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Amazing to see this.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Daily Discussion Thread - September 04,...</td>\n",
       "      <td>15</td>\n",
       "      <td>czfltt</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/czfltt...</td>\n",
       "      <td>109</td>\n",
       "      <td>1.567598e+09</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>I have a query! I want to make a payment in US...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I find this extremely strange. - Album on Imgu...</td>\n",
       "      <td>41</td>\n",
       "      <td>g62sa1</td>\n",
       "      <td>https://imgur.com/a/pOM8XM0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.587597e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>This has me curious, what are some documents o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kolkata horror: You should be raped, woman tel...</td>\n",
       "      <td>176</td>\n",
       "      <td>cn734h</td>\n",
       "      <td>https://timesofindia.indiatimes.com/city/kolka...</td>\n",
       "      <td>52</td>\n",
       "      <td>1.565220e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Fuck this, imagine a man saying this.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chandrayaan Moon Landing</td>\n",
       "      <td>503</td>\n",
       "      <td>d0ffgv</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/d0ffgv...</td>\n",
       "      <td>1401</td>\n",
       "      <td>1.567798e+09</td>\n",
       "      <td>ISRO's Chandrayaan 2 mission is a significant ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Experts at Home: \"Solar battery phook gayi\" 😂</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ambedkar Jayanti 2020</td>\n",
       "      <td>1298</td>\n",
       "      <td>g15qec</td>\n",
       "      <td>https://i.redd.it/rcf07o8ress41.jpg</td>\n",
       "      <td>126</td>\n",
       "      <td>1.586900e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Politics</td>\n",
       "      <td>No news channel have explained in detail to co...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Score      ID  \\\n",
       "0  When you have to cook and clean all by yoursel...    283  fuhcgu   \n",
       "1  TCS not to layoff any employees, but no salary...     95  g2gx0s   \n",
       "2  How come western countries are facing so many ...     24  g41vk7   \n",
       "3  EXCLUSIVE: Documents Show Modi Govt Building 3...   2072  fjx9ih   \n",
       "4  Startrail over Leh Palace - Leh Palace was bui...    215  epg8gi   \n",
       "5  Random Daily Discussion Thread - September 04,...     15  czfltt   \n",
       "6  I find this extremely strange. - Album on Imgu...     41  g62sa1   \n",
       "7  Kolkata horror: You should be raped, woman tel...    176  cn734h   \n",
       "8                           Chandrayaan Moon Landing    503  d0ffgv   \n",
       "9                              Ambedkar Jayanti 2020   1298  g15qec   \n",
       "\n",
       "                                                 URL  num_comments  \\\n",
       "0                https://i.redd.it/hyd2c2b3coq41.png            15   \n",
       "1  https://www.thenewsminute.com/article/tcs-not-...            31   \n",
       "2  https://www.reddit.com/r/india/comments/g41vk7...            35   \n",
       "3  https://www.huffingtonpost.in/entry/aadhaar-na...           314   \n",
       "4                https://i.redd.it/iwbeppdci3b41.jpg             6   \n",
       "5  https://www.reddit.com/r/india/comments/czfltt...           109   \n",
       "6                        https://imgur.com/a/pOM8XM0             9   \n",
       "7  https://timesofindia.indiatimes.com/city/kolka...            52   \n",
       "8  https://www.reddit.com/r/india/comments/d0ffgv...          1401   \n",
       "9                https://i.redd.it/rcf07o8ress41.jpg           126   \n",
       "\n",
       "     created_on                                               Body  Original  \\\n",
       "0  1.585980e+09                                                NaN      True   \n",
       "1  1.587079e+09                                                NaN     False   \n",
       "2  1.587301e+09  Having a discussion with family and I am unabl...     False   \n",
       "3  1.584440e+09                                                NaN     False   \n",
       "4  1.579190e+09                                                NaN     False   \n",
       "5  1.567598e+09  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...     False   \n",
       "6  1.587597e+09                                                NaN     False   \n",
       "7  1.565220e+09                                                NaN     False   \n",
       "8  1.567798e+09  ISRO's Chandrayaan 2 mission is a significant ...     False   \n",
       "9  1.586900e+09                                                NaN     False   \n",
       "\n",
       "              Flair                                           Comments  id  \n",
       "0              Food  Not for me. I always cooked my own food. The o...   0  \n",
       "1  Business/Finance  Why not defer the increments instead of outrig...   1  \n",
       "2          AskIndia  The lockdown enforced by the government was tr...   2  \n",
       "3          Politics     And then came corona! To fuck up their plans.    3  \n",
       "4       Photography                              Amazing to see this.    4  \n",
       "5     [R]eddiquette  I have a query! I want to make a payment in US...   5  \n",
       "6          AskIndia  This has me curious, what are some documents o...   2  \n",
       "7     [R]eddiquette             Fuck this, imagine a man saying this.    5  \n",
       "8     [R]eddiquette     Experts at Home: \"Solar battery phook gayi\" 😂    5  \n",
       "9          Politics  No news channel have explained in detail to co...   3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data['id']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Import nltk stopwords as done in the previous notebook as well\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk punctuation which will be removed from our texts as well\n",
    "# nltk.download('punkt')\n",
    "# PUNCT = nltk.corpus.stopwords.words('punkt')\n",
    "# print(PUNCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, I will be combining the data present in the body, Title and the Comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is giving me a very weird problem. \n",
    "All comparison with nan are returning to be false. np.NaN is not working so I am trying a different approach. I am comparing the value with float or str to determine the existence of null value. Float = Null. str = something is present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(len(data)):\n",
    "#     print(type(data.iloc[i]['Body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_on</th>\n",
       "      <th>Body</th>\n",
       "      <th>Original</th>\n",
       "      <th>Flair</th>\n",
       "      <th>Comments</th>\n",
       "      <th>id</th>\n",
       "      <th>Combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you have to cook and clean all by yoursel...</td>\n",
       "      <td>283</td>\n",
       "      <td>fuhcgu</td>\n",
       "      <td>https://i.redd.it/hyd2c2b3coq41.png</td>\n",
       "      <td>15</td>\n",
       "      <td>1.585980e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Food</td>\n",
       "      <td>Not for me. I always cooked my own food. The o...</td>\n",
       "      <td>0</td>\n",
       "      <td>When you have to cook and clean all by yoursel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCS not to layoff any employees, but no salary...</td>\n",
       "      <td>95</td>\n",
       "      <td>g2gx0s</td>\n",
       "      <td>https://www.thenewsminute.com/article/tcs-not-...</td>\n",
       "      <td>31</td>\n",
       "      <td>1.587079e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>Why not defer the increments instead of outrig...</td>\n",
       "      <td>1</td>\n",
       "      <td>TCS not to layoff any employees, but no salary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How come western countries are facing so many ...</td>\n",
       "      <td>24</td>\n",
       "      <td>g41vk7</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g41vk7...</td>\n",
       "      <td>35</td>\n",
       "      <td>1.587301e+09</td>\n",
       "      <td>Having a discussion with family and I am unabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>The lockdown enforced by the government was tr...</td>\n",
       "      <td>2</td>\n",
       "      <td>How come western countries are facing so many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXCLUSIVE: Documents Show Modi Govt Building 3...</td>\n",
       "      <td>2072</td>\n",
       "      <td>fjx9ih</td>\n",
       "      <td>https://www.huffingtonpost.in/entry/aadhaar-na...</td>\n",
       "      <td>314</td>\n",
       "      <td>1.584440e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Politics</td>\n",
       "      <td>And then came corona! To fuck up their plans.</td>\n",
       "      <td>3</td>\n",
       "      <td>EXCLUSIVE: Documents Show Modi Govt Building 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Startrail over Leh Palace - Leh Palace was bui...</td>\n",
       "      <td>215</td>\n",
       "      <td>epg8gi</td>\n",
       "      <td>https://i.redd.it/iwbeppdci3b41.jpg</td>\n",
       "      <td>6</td>\n",
       "      <td>1.579190e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Amazing to see this.</td>\n",
       "      <td>4</td>\n",
       "      <td>Startrail over Leh Palace - Leh Palace was bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Daily Discussion Thread - September 04,...</td>\n",
       "      <td>15</td>\n",
       "      <td>czfltt</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/czfltt...</td>\n",
       "      <td>109</td>\n",
       "      <td>1.567598e+09</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>I have a query! I want to make a payment in US...</td>\n",
       "      <td>5</td>\n",
       "      <td>Random Daily Discussion Thread - September 04,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I find this extremely strange. - Album on Imgu...</td>\n",
       "      <td>41</td>\n",
       "      <td>g62sa1</td>\n",
       "      <td>https://imgur.com/a/pOM8XM0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.587597e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>This has me curious, what are some documents o...</td>\n",
       "      <td>2</td>\n",
       "      <td>I find this extremely strange. - Album on Imgu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kolkata horror: You should be raped, woman tel...</td>\n",
       "      <td>176</td>\n",
       "      <td>cn734h</td>\n",
       "      <td>https://timesofindia.indiatimes.com/city/kolka...</td>\n",
       "      <td>52</td>\n",
       "      <td>1.565220e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Fuck this, imagine a man saying this.</td>\n",
       "      <td>5</td>\n",
       "      <td>Kolkata horror: You should be raped, woman tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chandrayaan Moon Landing</td>\n",
       "      <td>503</td>\n",
       "      <td>d0ffgv</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/d0ffgv...</td>\n",
       "      <td>1401</td>\n",
       "      <td>1.567798e+09</td>\n",
       "      <td>ISRO's Chandrayaan 2 mission is a significant ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Experts at Home: \"Solar battery phook gayi\" 😂</td>\n",
       "      <td>5</td>\n",
       "      <td>Chandrayaan Moon Landing ISRO's Chandrayaan 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ambedkar Jayanti 2020</td>\n",
       "      <td>1298</td>\n",
       "      <td>g15qec</td>\n",
       "      <td>https://i.redd.it/rcf07o8ress41.jpg</td>\n",
       "      <td>126</td>\n",
       "      <td>1.586900e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Politics</td>\n",
       "      <td>No news channel have explained in detail to co...</td>\n",
       "      <td>3</td>\n",
       "      <td>Ambedkar Jayanti 2020 No news channel have exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Recently went to my school excursion to kerela...</td>\n",
       "      <td>65</td>\n",
       "      <td>ep4ug0</td>\n",
       "      <td>https://i.redd.it/q2e48p8m3za41.jpg</td>\n",
       "      <td>22</td>\n",
       "      <td>1.579136e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Photography</td>\n",
       "      <td>whats with these forced dress codes in south i...</td>\n",
       "      <td>4</td>\n",
       "      <td>Recently went to my school excursion to kerela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hey Reddit, I'm Dhruv Rathee, Youtuber and Act...</td>\n",
       "      <td>368</td>\n",
       "      <td>84dbn1</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/84dbn1...</td>\n",
       "      <td>344</td>\n",
       "      <td>1.521062e+09</td>\n",
       "      <td>I make videos on various social, political and...</td>\n",
       "      <td>False</td>\n",
       "      <td>AMA</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>6</td>\n",
       "      <td>Hey Reddit, I'm Dhruv Rathee, Youtuber and Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Something doesn't look right but I can't put a...</td>\n",
       "      <td>209</td>\n",
       "      <td>dtusd6</td>\n",
       "      <td>https://i.redd.it/p4b2nwsdbnx31.jpg</td>\n",
       "      <td>44</td>\n",
       "      <td>1.573328e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Food</td>\n",
       "      <td>/r/DescriptionDesync</td>\n",
       "      <td>0</td>\n",
       "      <td>Something doesn't look right but I can't put a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the new skill , habit , or an activit...</td>\n",
       "      <td>24</td>\n",
       "      <td>g0ncyj</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g0ncyj...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.586827e+09</td>\n",
       "      <td>Apart from reading news all days , scrolling t...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Cooking just started yesterday</td>\n",
       "      <td>2</td>\n",
       "      <td>What are the new skill , habit , or an activit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MASAAN director Neeraj Ghaywan here. I'm here ...</td>\n",
       "      <td>209</td>\n",
       "      <td>3f10la</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/3f10la...</td>\n",
       "      <td>198</td>\n",
       "      <td>1.438207e+09</td>\n",
       "      <td>Thank you for taking time out guys. I *really*...</td>\n",
       "      <td>False</td>\n",
       "      <td>AMA</td>\n",
       "      <td>Your views on your Heroine richa chadda tweets...</td>\n",
       "      <td>6</td>\n",
       "      <td>MASAAN director Neeraj Ghaywan here. I'm here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Indian indie scene</td>\n",
       "      <td>15</td>\n",
       "      <td>fzxwhy</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fzxwhy...</td>\n",
       "      <td>43</td>\n",
       "      <td>1.586731e+09</td>\n",
       "      <td>So I was thinking of making a playlist w 30 so...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Shaitani zindagi- ITRA\\n\\nMarne ki aadatein- K...</td>\n",
       "      <td>2</td>\n",
       "      <td>Indian indie scene So I was thinking of making...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AMA with me! ( I'm Gul Panag)</td>\n",
       "      <td>73</td>\n",
       "      <td>5y7vw3</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/5y7vw3...</td>\n",
       "      <td>113</td>\n",
       "      <td>1.489009e+09</td>\n",
       "      <td>I'm done for now;-)\\nThank you for all the que...</td>\n",
       "      <td>False</td>\n",
       "      <td>AMA</td>\n",
       "      <td>What's the worst and best thing about Bollywood?</td>\n",
       "      <td>6</td>\n",
       "      <td>AMA with me! ( I'm Gul Panag) I'm done for now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Most airlines may go bankrupt by May-end, says...</td>\n",
       "      <td>105</td>\n",
       "      <td>fkow46</td>\n",
       "      <td>https://www.moneycontrol.com/news/business/com...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.584564e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>The owners just have to pull themselves from t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Most airlines may go bankrupt by May-end, says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mahashivarātri | Assam</td>\n",
       "      <td>143</td>\n",
       "      <td>f7qaqi</td>\n",
       "      <td>https://i.redd.it/qrlhjfj50gi41.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1.582392e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Interesting. Vaishnava marking for Shivaratri ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Mahashivarātri | Assam Interesting. Vaishnava ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How much yelling is normal for Indians? Is it ...</td>\n",
       "      <td>59</td>\n",
       "      <td>g2zwkc</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g2zwkc...</td>\n",
       "      <td>76</td>\n",
       "      <td>1.587150e+09</td>\n",
       "      <td>My Norwegian boyfriend says I yell too much, a...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Are you a guy or a girl though? Cuz being a gi...</td>\n",
       "      <td>2</td>\n",
       "      <td>How much yelling is normal for Indians? Is it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  Score      ID  \\\n",
       "0   When you have to cook and clean all by yoursel...    283  fuhcgu   \n",
       "1   TCS not to layoff any employees, but no salary...     95  g2gx0s   \n",
       "2   How come western countries are facing so many ...     24  g41vk7   \n",
       "3   EXCLUSIVE: Documents Show Modi Govt Building 3...   2072  fjx9ih   \n",
       "4   Startrail over Leh Palace - Leh Palace was bui...    215  epg8gi   \n",
       "5   Random Daily Discussion Thread - September 04,...     15  czfltt   \n",
       "6   I find this extremely strange. - Album on Imgu...     41  g62sa1   \n",
       "7   Kolkata horror: You should be raped, woman tel...    176  cn734h   \n",
       "8                            Chandrayaan Moon Landing    503  d0ffgv   \n",
       "9                               Ambedkar Jayanti 2020   1298  g15qec   \n",
       "10  Recently went to my school excursion to kerela...     65  ep4ug0   \n",
       "11  Hey Reddit, I'm Dhruv Rathee, Youtuber and Act...    368  84dbn1   \n",
       "12  Something doesn't look right but I can't put a...    209  dtusd6   \n",
       "13  What are the new skill , habit , or an activit...     24  g0ncyj   \n",
       "14  MASAAN director Neeraj Ghaywan here. I'm here ...    209  3f10la   \n",
       "15                                 Indian indie scene     15  fzxwhy   \n",
       "16                      AMA with me! ( I'm Gul Panag)     73  5y7vw3   \n",
       "17  Most airlines may go bankrupt by May-end, says...    105  fkow46   \n",
       "18                             Mahashivarātri | Assam    143  f7qaqi   \n",
       "19  How much yelling is normal for Indians? Is it ...     59  g2zwkc   \n",
       "\n",
       "                                                  URL  num_comments  \\\n",
       "0                 https://i.redd.it/hyd2c2b3coq41.png            15   \n",
       "1   https://www.thenewsminute.com/article/tcs-not-...            31   \n",
       "2   https://www.reddit.com/r/india/comments/g41vk7...            35   \n",
       "3   https://www.huffingtonpost.in/entry/aadhaar-na...           314   \n",
       "4                 https://i.redd.it/iwbeppdci3b41.jpg             6   \n",
       "5   https://www.reddit.com/r/india/comments/czfltt...           109   \n",
       "6                         https://imgur.com/a/pOM8XM0             9   \n",
       "7   https://timesofindia.indiatimes.com/city/kolka...            52   \n",
       "8   https://www.reddit.com/r/india/comments/d0ffgv...          1401   \n",
       "9                 https://i.redd.it/rcf07o8ress41.jpg           126   \n",
       "10                https://i.redd.it/q2e48p8m3za41.jpg            22   \n",
       "11  https://www.reddit.com/r/india/comments/84dbn1...           344   \n",
       "12                https://i.redd.it/p4b2nwsdbnx31.jpg            44   \n",
       "13  https://www.reddit.com/r/india/comments/g0ncyj...            19   \n",
       "14  https://www.reddit.com/r/india/comments/3f10la...           198   \n",
       "15  https://www.reddit.com/r/india/comments/fzxwhy...            43   \n",
       "16  https://www.reddit.com/r/india/comments/5y7vw3...           113   \n",
       "17  https://www.moneycontrol.com/news/business/com...            11   \n",
       "18                https://i.redd.it/qrlhjfj50gi41.jpg             2   \n",
       "19  https://www.reddit.com/r/india/comments/g2zwkc...            76   \n",
       "\n",
       "      created_on                                               Body  Original  \\\n",
       "0   1.585980e+09                                                NaN      True   \n",
       "1   1.587079e+09                                                NaN     False   \n",
       "2   1.587301e+09  Having a discussion with family and I am unabl...     False   \n",
       "3   1.584440e+09                                                NaN     False   \n",
       "4   1.579190e+09                                                NaN     False   \n",
       "5   1.567598e+09  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...     False   \n",
       "6   1.587597e+09                                                NaN     False   \n",
       "7   1.565220e+09                                                NaN     False   \n",
       "8   1.567798e+09  ISRO's Chandrayaan 2 mission is a significant ...     False   \n",
       "9   1.586900e+09                                                NaN     False   \n",
       "10  1.579136e+09                                                NaN     False   \n",
       "11  1.521062e+09  I make videos on various social, political and...     False   \n",
       "12  1.573328e+09                                                NaN     False   \n",
       "13  1.586827e+09  Apart from reading news all days , scrolling t...     False   \n",
       "14  1.438207e+09  Thank you for taking time out guys. I *really*...     False   \n",
       "15  1.586731e+09  So I was thinking of making a playlist w 30 so...     False   \n",
       "16  1.489009e+09  I'm done for now;-)\\nThank you for all the que...     False   \n",
       "17  1.584564e+09                                                NaN     False   \n",
       "18  1.582392e+09                                                NaN     False   \n",
       "19  1.587150e+09  My Norwegian boyfriend says I yell too much, a...     False   \n",
       "\n",
       "               Flair                                           Comments  id  \\\n",
       "0               Food  Not for me. I always cooked my own food. The o...   0   \n",
       "1   Business/Finance  Why not defer the increments instead of outrig...   1   \n",
       "2           AskIndia  The lockdown enforced by the government was tr...   2   \n",
       "3           Politics     And then came corona! To fuck up their plans.    3   \n",
       "4        Photography                              Amazing to see this.    4   \n",
       "5      [R]eddiquette  I have a query! I want to make a payment in US...   5   \n",
       "6           AskIndia  This has me curious, what are some documents o...   2   \n",
       "7      [R]eddiquette             Fuck this, imagine a man saying this.    5   \n",
       "8      [R]eddiquette     Experts at Home: \"Solar battery phook gayi\" 😂    5   \n",
       "9           Politics  No news channel have explained in detail to co...   3   \n",
       "10       Photography  whats with these forced dress codes in south i...   4   \n",
       "11               AMA                                         [deleted]    6   \n",
       "12              Food                              /r/DescriptionDesync    0   \n",
       "13          AskIndia                    Cooking just started yesterday    2   \n",
       "14               AMA  Your views on your Heroine richa chadda tweets...   6   \n",
       "15          AskIndia  Shaitani zindagi- ITRA\\n\\nMarne ki aadatein- K...   2   \n",
       "16               AMA  What's the worst and best thing about Bollywood?    6   \n",
       "17  Business/Finance  The owners just have to pull themselves from t...   1   \n",
       "18       Photography  Interesting. Vaishnava marking for Shivaratri ...   4   \n",
       "19          AskIndia  Are you a guy or a girl though? Cuz being a gi...   2   \n",
       "\n",
       "                                              Combine  \n",
       "0   When you have to cook and clean all by yoursel...  \n",
       "1   TCS not to layoff any employees, but no salary...  \n",
       "2   How come western countries are facing so many ...  \n",
       "3   EXCLUSIVE: Documents Show Modi Govt Building 3...  \n",
       "4   Startrail over Leh Palace - Leh Palace was bui...  \n",
       "5   Random Daily Discussion Thread - September 04,...  \n",
       "6   I find this extremely strange. - Album on Imgu...  \n",
       "7   Kolkata horror: You should be raped, woman tel...  \n",
       "8   Chandrayaan Moon Landing ISRO's Chandrayaan 2 ...  \n",
       "9   Ambedkar Jayanti 2020 No news channel have exp...  \n",
       "10  Recently went to my school excursion to kerela...  \n",
       "11  Hey Reddit, I'm Dhruv Rathee, Youtuber and Act...  \n",
       "12  Something doesn't look right but I can't put a...  \n",
       "13  What are the new skill , habit , or an activit...  \n",
       "14  MASAAN director Neeraj Ghaywan here. I'm here ...  \n",
       "15  Indian indie scene So I was thinking of making...  \n",
       "16  AMA with me! ( I'm Gul Panag) I'm done for now...  \n",
       "17  Most airlines may go bankrupt by May-end, says...  \n",
       "18  Mahashivarātri | Assam Interesting. Vaishnava ...  \n",
       "19  How much yelling is normal for Indians? Is it ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Combine'] = data['Title'] # Create a column combined\n",
    "count = 0\n",
    "for i in range(len(data)):\n",
    "    if type(data.loc[i]['Body']) != float:\n",
    "        data['Combine'][i] = data['Combine'][i] + ' ' + data['Body'][i]\n",
    "\n",
    "    if type(data.loc[i]['Comments']) != float:\n",
    "        data['Combine'][i] = data['Combine'][i] + ' ' + data['Comments'][i]\n",
    "\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1650 entries, 0 to 1649\n",
      "Data columns (total 12 columns):\n",
      "Title           1650 non-null object\n",
      "Score           1650 non-null int64\n",
      "ID              1650 non-null object\n",
      "URL             1650 non-null object\n",
      "num_comments    1650 non-null int64\n",
      "created_on      1650 non-null float64\n",
      "Body            635 non-null object\n",
      "Original        1650 non-null bool\n",
      "Flair           1650 non-null object\n",
      "Comments        1557 non-null object\n",
      "id              1650 non-null int64\n",
      "Combine         1650 non-null object\n",
      "dtypes: bool(1), float64(1), int64(3), object(7)\n",
      "memory usage: 143.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spitting in public now an offence under Disaster Management Act: MHA Der aae durust aae '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[34]['Combine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning our data that will be used as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_SPACES = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS = re.compile('[^0-9a-z #+_]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain symbols which add no analytical value to the data. Similarly, there certain areas where there are extra spaces or bracket spaces which are being being substituted by just one space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "        \n",
    "    '''\n",
    "\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_SPACES.sub(' ', text) \n",
    "    text = BAD_SYMBOLS.sub('', text) # Replace Bad Symbols which \n",
    "    text = text.replace('x', '')\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "data['Combine'] = data['Combine'].apply(clean_text)\n",
    "data['Combine'] = data['Combine'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cook clean peel onions make dough clean dishes...\n",
       "1    tcs layoff employees salary increments either ...\n",
       "2    come western countries facing many deaths numb...\n",
       "3    eclusive documents show modi govt building  de...\n",
       "4    startrail leh palace leh palace built th centu...\n",
       "5    random daily discussion thread september   am ...\n",
       "6    find etremely strange album imgur details atm ...\n",
       "7    kolkata horror raped woman tells girl shorts f...\n",
       "8    chandrayaan moon landing isros chandrayaan  mi...\n",
       "9    ambedkar jayanti  news channel eplained detail...\n",
       "Name: Combine, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Combine'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Feature Importance\n",
    "\n",
    "TFIDF =  Term Frequency–Inverse Document Frequency.\n",
    "It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the Tfidf vectorizer\n",
    "# I will be performing a hyperparameter tuning soon\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, \n",
    "                        min_df=5, \n",
    "                        norm = 'l2', \n",
    "                        encoding='latin-1', \n",
    "                        ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_on</th>\n",
       "      <th>Body</th>\n",
       "      <th>Original</th>\n",
       "      <th>Flair</th>\n",
       "      <th>Comments</th>\n",
       "      <th>id</th>\n",
       "      <th>Combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Hi /r/India, I’m Shankar. I’m an out Indian ga...</td>\n",
       "      <td>218</td>\n",
       "      <td>2b7pz6</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/2b7pz6...</td>\n",
       "      <td>322</td>\n",
       "      <td>1.405898e+09</td>\n",
       "      <td>I’m Shankar and I’m 22 years old. I came out p...</td>\n",
       "      <td>False</td>\n",
       "      <td>AMA</td>\n",
       "      <td>Can you rate the following in the order of aro...</td>\n",
       "      <td>6</td>\n",
       "      <td>hi r india im shankar im indian gay man living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>Relevance of Udemy courses in the job sector, ...</td>\n",
       "      <td>28</td>\n",
       "      <td>g4xetc</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g4xetc...</td>\n",
       "      <td>38</td>\n",
       "      <td>1.587433e+09</td>\n",
       "      <td>has anybody ever put any kind of udemy certif...</td>\n",
       "      <td>False</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Short answer: those certificates are BS.</td>\n",
       "      <td>2</td>\n",
       "      <td>relevance udemy courses job sector particularl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>At Delhi Haat, Delhi (3024x4032)</td>\n",
       "      <td>240</td>\n",
       "      <td>eo5lv3</td>\n",
       "      <td>https://i.redd.it/gxm4hpqkeka41.jpg</td>\n",
       "      <td>12</td>\n",
       "      <td>1.578958e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Did they walk with Gandhiji in his landmark ev...</td>\n",
       "      <td>4</td>\n",
       "      <td>delhi haat delhi  walk gandhiji landmark events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>They're just everywhere these days.</td>\n",
       "      <td>1899</td>\n",
       "      <td>bmu870</td>\n",
       "      <td>https://i.redd.it/lltlzq3ngbx21.png</td>\n",
       "      <td>147</td>\n",
       "      <td>1.557494e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>The Zomato guy's secretly eating your food.</td>\n",
       "      <td>5</td>\n",
       "      <td>theyre everywhere days zomato guys secretly ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>Hi, Norinder Mudi here. Ask me anything!</td>\n",
       "      <td>127</td>\n",
       "      <td>3ss2iv</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/3ss2iv...</td>\n",
       "      <td>278</td>\n",
       "      <td>1.447537e+09</td>\n",
       "      <td>Nomuste jonta!\\n\\nThis is Swagmohan. The guy w...</td>\n",
       "      <td>False</td>\n",
       "      <td>AMA</td>\n",
       "      <td>Do you think intolerance has increased since M...</td>\n",
       "      <td>6</td>\n",
       "      <td>hi norinder mudi ask anything nomuste jontathi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  Score      ID  \\\n",
       "1645  Hi /r/India, I’m Shankar. I’m an out Indian ga...    218  2b7pz6   \n",
       "1646  Relevance of Udemy courses in the job sector, ...     28  g4xetc   \n",
       "1647                   At Delhi Haat, Delhi (3024x4032)    240  eo5lv3   \n",
       "1648                They're just everywhere these days.   1899  bmu870   \n",
       "1649           Hi, Norinder Mudi here. Ask me anything!    127  3ss2iv   \n",
       "\n",
       "                                                    URL  num_comments  \\\n",
       "1645  https://www.reddit.com/r/india/comments/2b7pz6...           322   \n",
       "1646  https://www.reddit.com/r/india/comments/g4xetc...            38   \n",
       "1647                https://i.redd.it/gxm4hpqkeka41.jpg            12   \n",
       "1648                https://i.redd.it/lltlzq3ngbx21.png           147   \n",
       "1649  https://www.reddit.com/r/india/comments/3ss2iv...           278   \n",
       "\n",
       "        created_on                                               Body  \\\n",
       "1645  1.405898e+09  I’m Shankar and I’m 22 years old. I came out p...   \n",
       "1646  1.587433e+09   has anybody ever put any kind of udemy certif...   \n",
       "1647  1.578958e+09                                                NaN   \n",
       "1648  1.557494e+09                                                NaN   \n",
       "1649  1.447537e+09  Nomuste jonta!\\n\\nThis is Swagmohan. The guy w...   \n",
       "\n",
       "      Original          Flair  \\\n",
       "1645     False            AMA   \n",
       "1646     False       AskIndia   \n",
       "1647     False    Photography   \n",
       "1648     False  [R]eddiquette   \n",
       "1649     False            AMA   \n",
       "\n",
       "                                               Comments  id  \\\n",
       "1645  Can you rate the following in the order of aro...   6   \n",
       "1646          Short answer: those certificates are BS.    2   \n",
       "1647  Did they walk with Gandhiji in his landmark ev...   4   \n",
       "1648       The Zomato guy's secretly eating your food.    5   \n",
       "1649  Do you think intolerance has increased since M...   6   \n",
       "\n",
       "                                                Combine  \n",
       "1645  hi r india im shankar im indian gay man living...  \n",
       "1646  relevance udemy courses job sector particularl...  \n",
       "1647    delhi haat delhi  walk gandhiji landmark events  \n",
       "1648  theyre everywhere days zomato guys secretly ea...  \n",
       "1649  hi norinder mudi ask anything nomuste jontathi...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 3299)\n"
     ]
    }
   ],
   "source": [
    "# Extracting the features by fitting the Vectorizer on our Title data because that has the description of the post\n",
    "feat = tfidf.fit_transform(data['Combine']).toarray()\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to look at the most correlated words with each category and list them. I am gonna look at monograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flair 'AMA':\n",
      "Most correlated unigrams:\n",
      "\t. hi\n",
      "\t. anything\n",
      "\t. ask\n",
      "\t. questions\n",
      "\t. ama\n",
      "Most correlated bigrams:\n",
      "\t. ask us\n",
      "\t. us anything\n",
      "\t. hi reddit\n",
      "\t. answer questions\n",
      "\t. ask anything\n",
      "\n",
      "Flair 'AskIndia':\n",
      "Most correlated unigrams:\n",
      "\t. advice\n",
      "\t. dad\n",
      "\t. situation\n",
      "\t. afraid\n",
      "\t. family\n",
      "Most correlated bigrams:\n",
      "\t. ive seen\n",
      "\t. want know\n",
      "\t. feel like\n",
      "\t. work home\n",
      "\t. dont want\n",
      "\n",
      "Flair 'Business/Finance':\n",
      "Most correlated unigrams:\n",
      "\t. firms\n",
      "\t. emi\n",
      "\t. hdfc\n",
      "\t. mukesh\n",
      "\t. bank\n",
      "Most correlated bigrams:\n",
      "\t. credit card\n",
      "\t. mukesh ambani\n",
      "\t. share market\n",
      "\t. reliance jio\n",
      "\t. yes bank\n",
      "\n",
      "Flair 'Food':\n",
      "Most correlated unigrams:\n",
      "\t. restaurant\n",
      "\t. chutney\n",
      "\t. recipe\n",
      "\t. chicken\n",
      "\t. food\n",
      "Most correlated bigrams:\n",
      "\t. im trying\n",
      "\t. every day\n",
      "\t. couldnt find\n",
      "\t. dont eat\n",
      "\t. indian food\n",
      "\n",
      "Flair 'Non-Political':\n",
      "Most correlated unigrams:\n",
      "\t. rural\n",
      "\t. dads\n",
      "\t. found\n",
      "\t. bored\n",
      "\t. comics\n",
      "Most correlated bigrams:\n",
      "\t. im gonna\n",
      "\t. palghar lynching\n",
      "\t. amazon prime\n",
      "\t. india live\n",
      "\t. amid lockdown\n",
      "\n",
      "Flair 'Photography':\n",
      "Most correlated unigrams:\n",
      "\t. mm\n",
      "\t. beach\n",
      "\t. nikon\n",
      "\t. shot\n",
      "\t. oc\n",
      "Most correlated bigrams:\n",
      "\t. stay home\n",
      "\t. equipment nikon\n",
      "\t. one plus\n",
      "\t. da mm\n",
      "\t. nikon da\n",
      "\n",
      "Flair 'Policy/Economy':\n",
      "Most correlated unigrams:\n",
      "\t. gdp\n",
      "\t. govt\n",
      "\t. investments\n",
      "\t. nirmala\n",
      "\t. economy\n",
      "Most correlated bigrams:\n",
      "\t. health workers\n",
      "\t. https internetfreedomin\n",
      "\t. petrol diesel\n",
      "\t. indian economy\n",
      "\t. raghuram rajan\n",
      "\n",
      "Flair 'Politics':\n",
      "Most correlated unigrams:\n",
      "\t. sonia\n",
      "\t. removed\n",
      "\t. modi\n",
      "\t. arnab\n",
      "\t. muslims\n",
      "Most correlated bigrams:\n",
      "\t. home minister\n",
      "\t. arnab goswami\n",
      "\t. pm modi\n",
      "\t. rahul gandhi\n",
      "\t. john oliver\n",
      "\n",
      "Flair 'Science/Technology':\n",
      "Most correlated unigrams:\n",
      "\t. vpn\n",
      "\t. iit\n",
      "\t. develop\n",
      "\t. zoom\n",
      "\t. users\n",
      "Most correlated bigrams:\n",
      "\t. anyone else\n",
      "\t. covid virus\n",
      "\t. home affairs\n",
      "\t. ministry home\n",
      "\t. cow urine\n",
      "\n",
      "Flair 'Sports':\n",
      "Most correlated unigrams:\n",
      "\t. ipl\n",
      "\t. football\n",
      "\t. sports\n",
      "\t. cricket\n",
      "\t. cup\n",
      "Most correlated bigrams:\n",
      "\t. india pakistan\n",
      "\t. know people\n",
      "\t. one time\n",
      "\t. times india\n",
      "\t. world cup\n",
      "\n",
      "Flair '[R]eddiquette':\n",
      "Most correlated unigrams:\n",
      "\t. boop\n",
      "\t. askaway\n",
      "\t. beep\n",
      "\t. creator\n",
      "\t. bot\n",
      "Most correlated bigrams:\n",
      "\t. bot problem\n",
      "\t. bot bot\n",
      "\t. askaway creator\n",
      "\t. beep boop\n",
      "\t. discussion thread\n"
     ]
    }
   ],
   "source": [
    "# chisq2 statistical test\n",
    "N = 5    # Number of examples to be listed\n",
    "for f, i in sorted(category_labels.items()):\n",
    "    chi2_feat = chi2(feat, labels == i)\n",
    "    indices = np.argsort(chi2_feat[0])\n",
    "    feat_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [w for w in feat_names if len(w.split(' ')) == 1]\n",
    "    bigrams = [w for w in feat_names if len(w.split(' ')) == 2]\n",
    "    print(\"\\nFlair '{}':\".format(f))\n",
    "    print(\"Most correlated unigrams:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"Most correlated bigrams:\\n\\t. {}\".format('\\n\\t. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Food',\n",
       " 'Policy/Economy',\n",
       " 'Politics',\n",
       " 'AskIndia',\n",
       " 'Photography',\n",
       " 'Business/Finance',\n",
       " '[R]eddiquette',\n",
       " 'Non-Political',\n",
       " 'Science/Technology',\n",
       " 'AMA',\n",
       " 'Sports']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_list = list(category_labels.keys())\n",
    "flair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1402,) (1402,) (248,) (248,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting 20% of the data into train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Combine'], data['Flair'], test_size=0.15, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I randomized the training and testing data for better predictions. This is very important since the data has homogenous flairs for every 150 entries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our classifiers\n",
    "\n",
    "I will be building functions for different clasifiers. These functions will have a pipeline implemented for each model. This pipeline will first create an instance of the Count Vectorizer to create vectors of word counts and then it will also implement a TFID Transformer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14783)\t1\n",
      "  (0, 14548)\t1\n",
      "  (0, 3861)\t1\n",
      "  (0, 15483)\t1\n",
      "  (0, 825)\t1\n",
      "  (0, 16550)\t3\n",
      "  (0, 12174)\t1\n",
      "  (0, 10065)\t1\n",
      "  (0, 10560)\t2\n",
      "  (0, 8486)\t1\n",
      "  (0, 7021)\t1\n",
      "  (0, 16812)\t1\n",
      "  (0, 14513)\t1\n",
      "  (0, 4939)\t1\n",
      "  (0, 13223)\t1\n",
      "  (0, 5032)\t1\n",
      "  (0, 16623)\t2\n",
      "  (0, 12971)\t1\n",
      "  (0, 9130)\t1\n",
      "  (0, 10523)\t1\n",
      "  (0, 363)\t1\n",
      "  (1, 13095)\t1\n",
      "  (1, 13307)\t1\n",
      "  (1, 6102)\t1\n",
      "  (1, 14744)\t2\n",
      "  :\t:\n",
      "  (1401, 11945)\t3\n",
      "  (1401, 11939)\t1\n",
      "  (1401, 4695)\t1\n",
      "  (1401, 16293)\t1\n",
      "  (1401, 16211)\t1\n",
      "  (1401, 14669)\t2\n",
      "  (1401, 13100)\t1\n",
      "  (1401, 8560)\t1\n",
      "  (1401, 12503)\t2\n",
      "  (1401, 5485)\t1\n",
      "  (1401, 8974)\t1\n",
      "  (1401, 14251)\t1\n",
      "  (1401, 684)\t1\n",
      "  (1401, 3014)\t1\n",
      "  (1401, 7067)\t1\n",
      "  (1401, 5890)\t1\n",
      "  (1401, 2352)\t1\n",
      "  (1401, 10291)\t2\n",
      "  (1401, 8215)\t1\n",
      "  (1401, 5692)\t1\n",
      "  (1401, 2511)\t1\n",
      "  (1401, 66)\t1\n",
      "  (1401, 11998)\t1\n",
      "  (1401, 14592)\t1\n",
      "  (1401, 11310)\t1\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the TFID transformer\n",
    "count_vec = CountVectorizer()\n",
    "X_train_counts = count_vec.fit_transform(X_train)\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16812)\t0.11022236890576421\n",
      "  (0, 16623)\t0.1925209243808672\n",
      "  (0, 16550)\t0.42626574810284235\n",
      "  (0, 15483)\t0.17615073768910242\n",
      "  (0, 14783)\t0.23418542063815603\n",
      "  (0, 14548)\t0.22161409270279045\n",
      "  (0, 14513)\t0.14130361141634007\n",
      "  (0, 13223)\t0.1627944384691295\n",
      "  (0, 12971)\t0.12439900504259141\n",
      "  (0, 12174)\t0.13143004595762284\n",
      "  (0, 10560)\t0.46837084127631207\n",
      "  (0, 10523)\t0.23418542063815603\n",
      "  (0, 10065)\t0.13316908358352358\n",
      "  (0, 9130)\t0.10883179012579917\n",
      "  (0, 8486)\t0.12020278585316788\n",
      "  (0, 7021)\t0.20012326565000102\n",
      "  (0, 5032)\t0.15022311053376394\n",
      "  (0, 4939)\t0.17863243859721162\n",
      "  (0, 3861)\t0.23418542063815603\n",
      "  (0, 825)\t0.22161409270279045\n",
      "  (0, 363)\t0.19534386858520836\n",
      "  (1, 14744)\t0.4692741705095136\n",
      "  (1, 13478)\t0.22819142474839169\n",
      "  (1, 13308)\t0.1982366873500187\n",
      "  (1, 13307)\t0.2670309452384351\n",
      "  :\t:\n",
      "  (1401, 8215)\t0.15983661879241237\n",
      "  (1401, 7335)\t0.05020117357779233\n",
      "  (1401, 7180)\t0.1453138840129959\n",
      "  (1401, 7165)\t0.10554782093250241\n",
      "  (1401, 7067)\t0.15340353250776484\n",
      "  (1401, 6452)\t0.08756060440513805\n",
      "  (1401, 6211)\t0.07326963213195758\n",
      "  (1401, 6097)\t0.06672038043971774\n",
      "  (1401, 5890)\t0.1484136431231644\n",
      "  (1401, 5809)\t0.18422688324711833\n",
      "  (1401, 5692)\t0.15983661879241237\n",
      "  (1401, 5485)\t0.1224035081927307\n",
      "  (1401, 5437)\t0.17692240181452204\n",
      "  (1401, 4926)\t0.13526968076202583\n",
      "  (1401, 4695)\t0.10136143561668876\n",
      "  (1401, 3703)\t0.07970271841660515\n",
      "  (1401, 3014)\t0.13078232467582193\n",
      "  (1401, 2511)\t0.1689035446652819\n",
      "  (1401, 2352)\t0.15983661879241237\n",
      "  (1401, 945)\t0.0766134294660421\n",
      "  (1401, 684)\t0.11976966860450877\n",
      "  (1401, 680)\t0.09648089505963292\n",
      "  (1401, 539)\t0.08669487858773187\n",
      "  (1401, 505)\t0.10489960770080618\n",
      "  (1401, 66)\t0.1689035446652819\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the TFID transformer\n",
    "tfidf_trans = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_trans.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input Sequences\n",
    "pre_train = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Pipeline\n",
    "The first one that I am building is the Naive Bayes Classifier. The one most suitable for word counts is the multinomial variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classifier(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    nb_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', MultinomialNB()),\n",
    "                 ])\n",
    "    nb_fit.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = nb_fit.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', LogisticRegression()),\n",
    "                 ])\n",
    "    logreg.fit(X_train, y_train)     # Fitting the data to the trianing data\n",
    "\n",
    "    # Making Predictions on the test data\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    forest = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', RandomForestClassifier()),\n",
    "                 ])\n",
    "    forest.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = forest.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    svc_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', SVC()),\n",
    "                 ])\n",
    "    svc_fit.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = svc_fit.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vec.transform(X_test)\n",
    "X_test_tfidf = tfidf_trans.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06048387096774194"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = SVC()\n",
    "log_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = log_model.predict(X_test_tfidf)\n",
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Naive Bayes Classifier\n",
      "Model Accuracy: 0.43951612903225806\n",
      "Evaluate Random Forest Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.4274193548387097\n",
      "Evaluate Logistic Regression Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5645161290322581\n",
      "Evaluate SVC Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.06048387096774194\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate Naive Bayes Classifier\")\n",
    "nb_classifier(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Evaluate Random Forest Classifier\")\n",
    "random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Evaluate Logistic Regression Model\")\n",
    "log_reg(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Evaluate SVC Model\")\n",
    "svc(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model gives the best results and was working well in the flask app. Apart from that the model is not converging for some cases. However, Heroku results in a error that says that module is not found even though it is present there so I will be using SVC right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1,1, 10, 100], \n",
    "#               'gamma': [1,0.1,0.01,0.001],\n",
    "#               'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train_tfidf, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print best parameter after tuning \n",
    "# print(grid.best_params_) \n",
    "\n",
    "# # print how our model looks after hyper-parameter tuning \n",
    "# print(grid.best_estimator_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    svc_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', SVC(C=1, gamma=1, kernel='linear')),\n",
    "                 ])\n",
    "    svc_fit.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = svc_fit.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate SVC Model\n",
      "Model Accuracy: 0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate SVC Model\")\n",
    "svc(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a 62% accuracy which is the best so far and I will go with this for now. Heroku results in an error for this model as well. Hence, I will tune my Naive Bayes Model next. Here is the link to the error logs: [Click](https://stackoverflow.com/questions/61417803/modulenotfounderror-no-module-named-sklearn-svm-classes-on-heroku?noredirect=1#comment108648845_61417803)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_jobs': \n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.326, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.326, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.357, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.257, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.286, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.264, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.250, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.350, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.279, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.321, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.177, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.163, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.193, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.143, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.150, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.164, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.121, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.207, total=   0.6s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.179, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.171, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.234, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.241, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.186, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.186, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.171, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.171, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.136, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.250, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.171, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.236, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.340, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.390, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.421, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.293, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.307, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.286, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.300, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.371, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.300, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.371, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.227, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.262, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.293, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.193, total=   0.6s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.214, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.221, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.186, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.286, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.214, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.264, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.248, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.270, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.236, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.207, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.193, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.186, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.157, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.271, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.193, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.286, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.489, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.518, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.500, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.521, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.521, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.464, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.479, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.464, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.511, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.518, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.493, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.507, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.529, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.529, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.493, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.464, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.383, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.426, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.336, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.314, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.386, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.357, total=   0.3s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.321, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.386, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.393, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.447, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.496, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.493, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.457, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.471, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.464, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.407, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.471, total=   0.2s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.414, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.414, total=   0.1s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.426, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.489, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.450, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.429, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.443, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.414, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.450, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.414, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.407, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.383, total=   0.8s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.411, total=   0.5s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.336, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.307, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.357, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.336, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.379, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.386, total=   0.4s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.429, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.582, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.582, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.529, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.521, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.500, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.479, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.514, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.500, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.546, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.560, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.536, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.479, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.500, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.521, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.362, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.390, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.321, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.271, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.357, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.321, total=   0.6s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.336, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.400, total=   0.6s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.343, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.525, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.532, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.536, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.521, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.521, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.493, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.471, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.521, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.507, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.525, total=   0.6s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.525, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.471, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.479, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.493, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.493, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.486, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.493, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.383, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.390, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.321, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.271, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.357, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.321, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.321, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.393, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.364, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.379, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.553, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.532, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.529, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.564, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.550, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.525, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.532, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.543, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.521, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.479, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.550, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.493, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.397, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.447, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.336, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.407, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.379, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.414, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.518, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.525, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.550, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.557, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.507, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.500, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.479, total=   0.2s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.536, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.457, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.457, total=   0.1s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.511, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.504, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.536, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.514, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.471, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.507, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.464, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.450, total=   0.5s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.390, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.447, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.343, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.343, total=   0.3s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.400, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.371, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.371, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.414, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.414, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.560, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.546, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.529, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.529, total=   0.2s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.514, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.493, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.536, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.564, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.514, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.596, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.582, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.529, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.543, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.550, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.479, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.486, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.536, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.564, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.521, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.383, total=   0.3s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.426, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.329, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.350, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.393, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.371, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.379, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.400, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.539, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.553, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.536, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.529, total=   0.2s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.521, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.450, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.471, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.500, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.550, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.500, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.567, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.546, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.550, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.536, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.543, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.457, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.486, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.529, total=   0.7s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.557, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.507, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.390, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.433, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.343, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.350, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.386, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.364, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.379, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.379, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.393, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.496, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.504, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.536, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.543, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.521, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.471, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.550, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.536, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.479, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.539, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.496, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.529, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.550, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.521, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.479, total=   0.7s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.500, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.564, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.521, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.543, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.397, total=   0.3s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.447, total=   0.3s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.343, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.357, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.421, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.357, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.386, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.421, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.443, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.525, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.525, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.543, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.550, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.529, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.464, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.486, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.550, total=   0.1s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.564, total=   0.2s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.471, total=   0.2s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.525, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.518, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.543, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.550, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.521, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.479, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.479, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.536, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.500, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.486, total=   0.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.383, total=   0.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.447, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.350, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.357, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.400, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.371, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.364, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.414, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.436, total=   0.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.429, total=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "               AMA     0.7826    0.9000    0.8372        20\n",
      "          AskIndia     0.3125    0.6818    0.4286        22\n",
      "  Business/Finance     0.6667    0.6400    0.6531        25\n",
      "              Food     0.5455    0.6000    0.5714        20\n",
      "     Non-Political     0.4000    0.1818    0.2500        22\n",
      "       Photography     0.9048    0.6333    0.7451        30\n",
      "    Policy/Economy     0.8333    0.3704    0.5128        27\n",
      "          Politics     0.2821    0.7857    0.4151        14\n",
      "Science/Technology     0.3333    0.3158    0.3243        19\n",
      "            Sports     0.9000    0.8182    0.8571        22\n",
      "     [R]eddiquette     0.8182    0.3333    0.4737        27\n",
      "\n",
      "          accuracy                         0.5565       248\n",
      "         macro avg     0.6163    0.5691    0.5517       248\n",
      "      weighted avg     0.6481    0.5565    0.5615       248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = GridSearchCV(nb_fit, param_grid, cv=10, scoring='accuracy', verbose=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test), digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "Pipeline(memory=None,\n",
      "         steps=[('vect',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=None, min_df=1,\n",
      "                                 ngram_range=(1, 2), preprocessor=None,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('tfidf',\n",
      "                 TfidfTransformer(norm='l1', smooth_idf=True,\n",
      "                                  sublinear_tf=False, use_idf=True)),\n",
      "                ('clf',\n",
      "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# print best parameter after tuning \n",
    "print(clf.best_params_) \n",
    "\n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(clf.best_estimator_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redifining and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classifier(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    nb_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MultinomialNB(alpha=0.01)),\n",
    "                 ])\n",
    "    nb_fit.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = nb_fit.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Naive Bayes Classifier\n",
      "Model Accuracy: 0.5201612903225806\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate Naive Bayes Classifier\")\n",
    "nb_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid which will include the parameters we will be testing\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Look at the parameter list\n",
    "from pprint import pprint\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" TAKES TIME TO EXECUTE SO SHOULDN'T EXECUTE AGAIN\\n# Use the random grid to search for best hyperparameters\\n\\n# First create the base model to tune\\nrf = RandomForestClassifier()\\n\\n# Random search of parameters, using 3 fold cross validation, \\n# search across 100 different combinations, and use all available cores\\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\\n\\nrf_random.fit(X_train_tfidf, y_train)\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' TAKES TIME TO EXECUTE SO SHOULDN'T EXECUTE AGAIN\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train_tfidf, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate Model performance\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=test_labels)\n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing test data\n",
    "# X_test_counts = count_vec.transform(X_test)\n",
    "# X_test_tfidf = tfidf_trans.transform(X_test_counts)\n",
    "# base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "# base_model.fit(X_train_tfidf, y_train)\n",
    "# base_accuracy = evaluate(base_model, X_test_tfidf , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_random = rf_random.best_estimator_\n",
    "# random_accuracy = evaluate(best_random, X_test_tfidf , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still get a 52% accuracy so I will go for an SVC model instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model for Deployment\n",
    "Joblib is part of the SciPy ecosystem and provides utilities for pipelining Python jobs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('model',\n",
       "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', MultinomialNB(alpha=0.01)),\n",
    "                 ])\n",
    "\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.sav']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "filename = 'final_model.sav'\n",
    "joblib.dump(nb_classifier, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found out certain reasons for my model performance. While some issues are there with the method of implementation, and the model type and parameters, there are issues which can also be attributed to the data quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "\n",
    "The data quality issues is one of the biggest issues with this current model. Some issues that I came across were:-\n",
    "* Unigram failing in the politics flair. When I redownloaded my data on 22rd of April, the Arnab and Sonia Gandhi incident had happened which included a words like car, stones, Italy etc. These words are generally not associated with politics but because of this incident, they came out on top and messed with the analysis. \n",
    "* Secondly, I scrolled through reddit for hours to understand the data and thread pattern. An issue I witnessed there was that flairs like non-political often have clearly political tones and words like BJP, Hindu, Muslim etc. This results in an inaccurate representation. Similarly, AMA and AskIndia also have similar issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I think something that could make the analysis better is exploring the url feature more. On posts which do not have a body, the url redirects to the additional content. If I can inculcate this data, then my features could become richher. \n",
    "\n",
    "* Secondly, the way my data is arranged is that there are 150 posts of each type one after the other. So, when I randomly sample it as testing and training data, there is skewness in the flairs and they aren't equally present in the training data which makes the analysis a little poor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple but useful technique for text classification tasks. It is also a frequency probability estimate. The above issues I mentioned reduce the model performance because the word frequencies get radically affected due to the above reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
